{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring typing behaviour: II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors\n",
    "import sklearn.manifold, sklearn.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "import pymc as mc\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC in practice: sampling issues\n",
    "<a id=\"sampling\"> </a>\n",
    "\n",
    "The **great thing** about MCMC approaches is that you can basically write down your model and then run inference directly. There is no need to derive complex approximations, or to restrict ourselves to limited models for which we can compute answers analyitically.\n",
    "\n",
    "The **bad thing** about MCMC approaches is that, even though it will do the \"right thing\" *asymptotically*, the choice of sampling strategy has a very large influence for the kind of sample runs that are practical to execute.\n",
    "\n",
    "Bayesian inference should depend only on the priors and the evidence observed; but MCMC approaches also depend on the sampling strategy used to approximate the posterior. \n",
    "\n",
    "MCMC allows us to use distributions *we can't even sample from directly*. First we couldn't calculate the evidence P(B), so we integrated; but we couldn't solve the integral, so we sampled; but then we couldn't sample from the distribution so we used MCMC. It's a very general approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection sampling\n",
    "\n",
    "The simplest way to perform Monte Carlo sampling for a distribution we can't sample directly is to do **rejection sampling**. We have a distribution we want to sample given by a pdf $p(x)$, and instead sample from an easy distribution $q(x)$, (usually uniform, i.e. a box) where $q(x)>p(x) \\forall x$. Then, we draw a new sample $x_q$ from $q(x)$ (horizontal sample) and then sample uniformly from $x_s = [0,x_q]$ (vertical sample) and see if $x_s<f(x_q)$. If we so we keep it as a draw from the distribution, otherwise we reject it.\n",
    "\n",
    "This is easy to implement, but works very poorly in *high dimensions* because the rejection rate increases exponentially with increasing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rejection_sample(p,interval_x, interval_y, n):\n",
    "    xs = np.random.uniform(interval_x[0], interval_x[1],(n,))\n",
    "    ys = np.random.uniform(interval_y[0], interval_y[1],(n,))\n",
    "    kept  = p(xs)>ys\n",
    "    return kept, np.logical_not(kept), xs, ys\n",
    "\n",
    "def odd_pdf(x):\n",
    "    return np.sqrt(1-x**2)\n",
    "\n",
    "kept, rejected, xs, ys = rejection_sample(odd_pdf, [-1,1], [0,1], 200)\n",
    "plt.plot(xs[kept], ys[kept], 'r.')\n",
    "plt.plot(xs[rejected], ys[rejected], 'k.')\n",
    "\n",
    "xf = np.linspace(-1,1,100)\n",
    "plt.plot(xf,cos_pdf(xf), '--')\n",
    "\n",
    "print \"Fraction under the curve: %.2f\" % (np.sum(kept) / float(len(xs)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings\n",
    "Metropolis-Hastings (or just plain Metropolis) takes a different approach, and is able to work in high-dimensional spaces. Metropolis also uses an auxiliary distribution $q(x)$, but it uses this to **wander around** in the distribution space, accepting jumps to new positions based on both $q(x)$ and $p(x)$.  This random walk (a **Markov chain**, because we make a random jump conditioned only on where we currently are) is a the \"Markov Chain\" bit of \"Markov Chain Monte Carlo\".\n",
    "\n",
    "We just take our current position $x$, and propose a new position $x^\\prime = x + x_q$, where $x_q$ is a random sample drawn from $q(x)$. This makes local steps in the space of the probability density. If $q(x)$ has a simple, symmetric form (e.g. is Gaussian), there is a very simple formula to decide whether to accept or reject a step from $p(x)$ to a new candidate position $p(x^\\prime)$:\n",
    "$$\n",
    "p(\\text{accept}) = \\begin{cases} p(x^\\prime)/p(x), & p(x)>=p(x^\\prime) \\\\  1, & p(x)<p(x^\\prime) \\end{cases} $$\n",
    "\n",
    "The asymmetric case is only slightly more involved, but it is very unusual to need to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def metropolis(p,q,x_init,n):\n",
    "    x = x_init\n",
    "    \n",
    "    samples = []\n",
    "    rejected = [] # we only keep the rejected samples to plot them later\n",
    "    for i in range(n):\n",
    "        # find a new candidate spot to jump to\n",
    "        x_prime = x + q()\n",
    "        # if it's better, go right away\n",
    "        if p(x_prime)>p(x):\n",
    "            x = x_prime\n",
    "            samples.append(x_prime)            \n",
    "        else:\n",
    "            # if not, go with probability proportional to the\n",
    "            # ratio between the new point and the current one\n",
    "            pa = p(x_prime)/p(x)\n",
    "            if np.random.uniform(0,1)<pa:\n",
    "                x = x_prime\n",
    "                samples.append(x_prime)\n",
    "            else:\n",
    "                rejected.append(x_prime)\n",
    "                \n",
    "    return np.array(samples), np.array(rejected)\n",
    "\n",
    "\n",
    "A = np.array([[0.15, 0.9], [-0.1, 2.5]])\n",
    "p = lambda x:scipy.stats.multivariate_normal(mean=[0,0], cov=A).pdf(x)\n",
    "q = lambda: np.random.normal(0,0.5,(2,))\n",
    "accept, reject = metropolis(p,q,[0.1, 0.3], 200)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(accept[:,0], accept[:,1])\n",
    "plt.plot(accept[:,0], accept[:,1], 'b.')\n",
    "plt.plot(reject[:,0], reject[:,1], 'r.')\n",
    "x,y = np.meshgrid(np.linspace(-5,5,30), np.linspace(-4,4,30))\n",
    "plt.imshow(p(np.dstack([x,y])), extent=[-4,4,-4,4], cmap='viridis')\n",
    "plt.grid(\"off\")\n",
    "plt.title(\"MCMC sampling with Metropolis-Hastings\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run the chain for longer, and plot the trace and the histogram of the variable\n",
    "accept, reject = metropolis(p,q,[0.1, 0.3], 2000)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(accept[:,0])\n",
    "plt.title(\"Trace for $x$\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(accept[:,0], bins=20)\n",
    "plt.title(\"Histogram of $x$\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(accept[:,1])\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Trace for $y$\")\n",
    "plt.plot(accept[:,0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Histogram of $y$\")\n",
    "plt.hist(accept[:,0], bins=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others\n",
    "There are many other MCMC samplers, such as:\n",
    "* **Gibbs** samplers, which are very efficient when we can sample from the marginal distribution (i.e. from one dimension of a distribution at a time), but not from the joint directly.\n",
    "* **Hamiltonian** samplers, which extend Metropolis-like steps with \"virtual physics\" which pushes the samples in sensible directions (i.e. not down the gradient of the function!)\n",
    "* **Slice** samplers, which are very clever and efficient, but only work for 1-dimensional (univariate) distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burn-in and thinning\n",
    "MCMC tries to draw **independent, unbiased** samples from the posterior, but the sampling process (like Metropolis), is not inherently unbiased. For example, successive samples in a random walk are correlated and obviously not independent. \n",
    "\n",
    "And although the Markov Chain approach (under fairly relaxed assumptions) will asympotically sample from all of the posterior, if the random walk starts off very far from the bulk of the distribution, it will \"wander in the wilderness\" for some time before reaching significant probability density. This means early samples from the distribution might be unreasonably dense in very low probability regions in the posterior. How \"good\" the Markov chain is at sampling from the posterior is called **mixing**; some MCMC setups may mix very badly until they get warmed up.\n",
    "\n",
    "To mitigate these two common issues, there are a couple of standard tricks: \n",
    "* **Burn-in**, which ignores the first $n$ samples from an MCMC draw, to make sure the chain is mixing well. Typically, several thousand samples might be ignored.\n",
    "* **Thinnning**, which takes one sample from every $k$ consecutive samples from the chain, to reduce correlation. Values of raound 5-50 are common.\n",
    "\n",
    "Tuning these is a matter of art!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Burn-in and thinning plot\n",
    "y = np.random.normal(0,1,(500,))\n",
    "x = np.arange(len(y))\n",
    "\n",
    "burn = 100\n",
    "thin = 4\n",
    "plt.plot(x[0:burn], y[0:burn], 'r:')\n",
    "plt.plot(x[burn::thin], y[burn::thin], 'ko')\n",
    "plt.plot(x[burn:], y[burn:], 'g:')\n",
    "\n",
    "plt.axvline(burn, c='r')\n",
    "plt.text(15,2.5,\"Burn-in period\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in learning more about MCMC, David Mackay's [book chapter](http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/356.384.pdf) is a good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling\n",
    "<a id=\"bigram\"> </a>\n",
    "\n",
    "In the keystroke event datasets, we are predicting a sequence of discrete symbols, which has very regular and repetitive structure. Capturing the regularities in symbol sequences is **language modelling**. In traditional text entry, a language model captures natural language to improve typing performance. We'll look at using a language model to predict keystroke data for the coding and navigating tasks that we have captured, using standard language modelling approaches.\n",
    "\n",
    "\n",
    "## Bigram model\n",
    "We can build a very simple probabilistic language model using *n-grams*, where we predict the next occurence of a symbol given some previous sequence of $n$ symbols. **This is a Markov assumption; that the current state depends only on the previous state** (a fixed sequence of $n$ previous symbols can be bundled into a single previous state for the sake of argument).\n",
    "\n",
    "A very basic model is a character-level bigram model, where we model the probability of a character $c_n$ given a previously seen character $c_{n-1}$, $p(c_n|c_{n-1})$. \n",
    "\n",
    "The probability distribution can be inferred from data simply by counting the occurence of each pair $(c_{n-1}, c_{n})$ and storing the result in a matrix which has dimensions $d \\times d$ where $d$ is the number of distinct characters. If we then normalise by dividing each row by the total count of that character, we approximate the probability distribution of the language. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerify(string):\n",
    "    # remove all but letters and space (note that this is not a very efficient way to do this process)\n",
    "    # and then convert to 0=space, 1-27 = a-z\n",
    "    filtered_string = [max(1+ord(c.lower()) - ord('a'), 0) for c in string if c.isalpha() or c.isspace()]\n",
    "    return filtered_string\n",
    "\n",
    "def learn_bigram(string):\n",
    "    # return a matrix with the bigram counts from string, including only letters and whitespace\n",
    "    coded = numerify(string)\n",
    "    joint = np.zeros((27,27))\n",
    "    # iterate over sequential pairs\n",
    "    for prev, this in zip(coded[:-1], coded[1:]):\n",
    "        joint[prev, this] += 1\n",
    "    # note that we add on an epsilon to avoid dividing by zero!\n",
    "    bigram = joint.T / (np.sum(joint, axis=0)+1e-6)\n",
    "    return bigram.T, joint.T\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/macbeth.txt\") as f:\n",
    "    macbeth_bigram, macbeth_joint = learn_bigram(f.read())\n",
    "\n",
    "with open(\"data/metamorphosis.txt\") as f:\n",
    "    metamorphosis_bigram, metamorphosis_joint = learn_bigram(f.read())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The joint distribution\n",
    "plt.imshow(macbeth_joint, interpolation='nearest', cmap=\"viridis\")\n",
    "plt.xticks(range(27), ' abcdefghijklmnopqrstuvwxyz')\n",
    "plt.yticks(range(27), ' abcdefghijklmnopqrstuvwxyz')\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The conditional distributions\n",
    "plt.imshow(macbeth_bigram, interpolation='nearest', cmap=\"viridis\")\n",
    "plt.xticks(range(27), ' abcdefghijklmnopqrstuvwxyz')\n",
    "plt.yticks(range(27), ' abcdefghijklmnopqrstuvwxyz')\n",
    "plt.grid('off')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(metamorphosis_bigram, interpolation='nearest', cmap=\"viridis\")\n",
    "plt.xticks(range(27), ' abcdefghijklmnopqrstuvwxyz')\n",
    "plt.yticks(range(27), ' abcdefghijklmnopqrstuvwxyz')\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log probabilities\n",
    "\n",
    "The probability of multiple **independent** random variables taking on a set of values can be computed from the product:\n",
    "$$p(x,y,z) = p(x)p(y)p(z)$$\n",
    "and in general\n",
    "$$p(x_1, \\dots, x_n) = \\prod_{i=1}^{n} x_i$$\n",
    "\n",
    "We often have to have to compute such products, but to multiply lots of values $<1$ leads to numerical issues. Instead, we often prefer to manipluate *log probabilities*, which can be summed instead of multiplied:\n",
    "$$\\log p(x_1, \\dots, x_n) = \\sum_{i=1}^{n} \\log p(x_i)$$\n",
    "\n",
    "This is simply a numerical conveience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def log_likelihood_bigram(string, bigram):\n",
    "    symbols = numerify(string)\n",
    "    llik = 0\n",
    "    # we sum the log probabilities to avoid numerical underflow\n",
    "    for prev, this in zip(symbols[:-1], symbols[1:]):\n",
    "        llik += np.log(bigram[prev, this]+1e-8) \n",
    "    return llik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def compare_logp(fname):\n",
    "    print fname\n",
    "\n",
    "    with open(fname) as f:\n",
    "        text = f.read()\n",
    "        mb_llik = log_likelihood_bigram(text, macbeth_bigram) / len(text)\n",
    "        mm_llik = log_likelihood_bigram(text, metamorphosis_bigram) / len(text)\n",
    "        diff = mb_llik - mm_llik\n",
    "        print \"\\tlogp Macbeth: % 10.2f\\t logp Metamorphosis:% 10.2f\\t Difference logp:% 10.2f\" % (mb_llik, mm_llik, diff)\n",
    "        if diff>0:\n",
    "            print \"\\tModel favours: Macbeth\"            \n",
    "        else:\n",
    "            print \"\\tModel favours: Metamorphosis\"\n",
    "\n",
    "    print\n",
    "    \n",
    "compare_logp(\"data/macbeth.txt\")\n",
    "compare_logp(\"data/metamorphosis.txt\")\n",
    "compare_logp(\"data/romeo_juliet.txt\")\n",
    "compare_logp(\"data/the_trial.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "<a id=\"imputation\"> </a>\n",
    "\n",
    "In PyMC, variables can be **observed** (fixed) or **unobserved** (random). PyMC cycles through the array of known values for the **observed** variables and updates the rest of the graph.\n",
    "\n",
    "But what if you want to ask \"what if?\"-style question? For example, if you knew the last two key codes and timings, what is the distribution over the possible times for the *next* key? \n",
    "\n",
    "PyMC implements this using **imputation**, where certain missing values in an observed variable can be inferred (*imputed*) from the rest of the model. **Masked arrays** are used to implement imputation; these allow arrays to have \"blank\" values, that PyMC can fill in automatically.\n",
    "\n",
    "This approach creates one new random variable per missing data item; this can create very large models if you are not careful!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Example, using the linear regression model from the last section:\n",
    "import numpy.ma as ma # masked array support\n",
    "\n",
    "\n",
    "## generate the data for the regression\n",
    "x = np.sort(np.random.uniform(0,20, (50,)))\n",
    "m = 2\n",
    "c = 15\n",
    "# Add on some measurement noise, with std. dev. 3.0\n",
    "epsilon = data = np.random.normal(0,3, x.shape)\n",
    "y = m * x + c + epsilon\n",
    "\n",
    "## Now the imputation; we will try and infer missing some missing values of y (we still have the corresponding x)\n",
    "## mark last three values of y invalid\n",
    "y_impute[-3:] = 0\n",
    "y_impute = ma.masked_equal(y_impute,0)\n",
    "print \"Y masked for imputation:\", y_impute # we will see the last three entries with --\n",
    "\n",
    "# create the model (exactly as before, except we switch \"y_impute\" for \"y\")\n",
    "m_unknown = mc.Normal('m', 0, 0.01)\n",
    "c_unknown = mc.Normal('c', 0, 0.001)\n",
    "precision = mc.Uniform('precision', lower=0.001, upper=10.0)\n",
    "x_obs = mc.Normal(\"x_obs\", 0, 1, value=x, observed=True)\n",
    "@mc.deterministic(plot=False)\n",
    "def line(m=m_unknown, c=c_unknown, x=x_obs):\n",
    "    return x*m+c\n",
    "y_obs =  mc.Normal('y_obs', mu=line, tau=precision, value=y_impute, observed=True)\n",
    "model = mc.Model([m_unknown, c_unknown, precision, x_obs, y_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample from the distribution\n",
    "mcmc = mc.MCMC(model)\n",
    "mcmc.sample(iter=10000, burn=2000, thin=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now we will have three entries in the y_obs trace from this run\n",
    "y_trace = mcmc.trace('y_obs')[:]\n",
    "\n",
    "## the original data\n",
    "plt.plot(x[:-3],y[:-3], '.')\n",
    "plt.plot(x[-3:],y[-3:], 'go')\n",
    "plt.plot(x,x*m+c, '-')\n",
    "\n",
    "# samples from posterior predicted for the missing values of y\n",
    "plt.plot(np.tile(x[-3], (len(y_trace[:,0]), 1)), y_trace[:,0],  'r.', alpha=0.01)\n",
    "plt.plot(np.tile(x[-2], (len(y_trace[:,1]), 1)), y_trace[:,1],  'r.', alpha=0.01)\n",
    "plt.plot(np.tile(x[-1], (len(y_trace[:,2]), 1)), y_trace[:,2],  'r.', alpha=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, while it makes sense to be able to infer $x$ given $y$, as well as the $y$ given $x$ we just did, PyMC cannot automatically infer variables in this manner. It can only infer the \"forward\" path in the graph. In theory, if all the determinstic functions (like the line function) were invertible, then this reverse inference could be performed automatically without changing the model.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# Challenge\n",
    "<a id=\"challenge\"> </a>\n",
    "\n",
    "### Task: Build a better model of typing behaviour, and then test if you can predict inter-key times using imputation.\n",
    "\n",
    "Extend the MCMC model to include key type information (for example, space+enter, punctiuation, numbers, letters). You choose the categories and write code to classify the keystrokes. There is a [list of keycodes here](https://css-tricks.com/snippets/javascript/javascript-keycodes/). **Only choose at most three or four categories of keystroke, or your model will never converge in a reasonable time**.\n",
    "\n",
    "Build a PyMC model that models the time to the next keystroke, dependent on:\n",
    "\n",
    "* The previous **keystroke code category** and the current **keystroke code category**\n",
    "* The previous keystroke press duration and the current keystroke press duration\n",
    "* The time between the previous keystroke and this one\n",
    "* The user ID\n",
    "\n",
    "This model will be something like:\n",
    "\n",
    "    if you just pressed a whitespace key and then you pressed a directional key, and you are user 202, and you held the last key for 104ms, then the next key will happen in x milliseconds.\n",
    "\n",
    "Use all of the keystroke data you have. Select the last three elements of the keylog, and try imputing the inter-key time.\n",
    "\n",
    "Plot a graph showing the predictive posterior against the true values.\n",
    "\n",
    "## Tips\n",
    "* Start off with a very simple model, and add variables one at a time. You might start with just a model depending on the user ID, and work from there, or start with the duration of the current keycode only, and work from there.\n",
    "* You might want to condense the key code categories and the user id into a **single** categorical variable to make the model easier to write down.\n",
    "* If you find you're not getting a good model, try limiting to a single user to begin with, and then expand to multiple users\n",
    "* For discrete variables (e.g. keycode category, user_id) use observed Categorical variables\n",
    "* For the timings, try using a Gamma distribution; you could also use a log-normal distribution, but you will have to implement the log transform yourself (not hard, but may require some online research [i.e. check StackOverflow!])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "## the variables in the model should go in the list passed to Model\n",
    "model = pymc.Model([])\n",
    "\n",
    "## see the graphical representation of the model\n",
    "show_dag(model)\n",
    "\n",
    "## Construct a sampler\n",
    "mcmc = pymc.MCMC(model)\n",
    "\n",
    "## Sample from the result; you should try changing the number of iterations\n",
    "mcmc.sample(iter=100000, burn=2000, thin=5)\n",
    "\n",
    "## Use the trace methods from pymc to explore the distribution of values\n",
    "\n",
    "## Plot the predictive distributions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
