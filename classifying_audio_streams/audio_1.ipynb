{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification of Audio Streams: Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "#import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from IPython.core.display import HTML\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jslog import js_key_update\n",
    "# This code logs keystrokes IN THIS JUPYTER NOTEBOOK WINDOW ONLY (not any other activity)\n",
    "# (don't type your passwords in this notebook!)\n",
    "# Log file is ../jupyter_keylog.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "function push_key(e,t,n){var o=keys.push([e,t,n]);o>500&&(kernel.execute(\"js_key_update([\"+keys+\"])\"),keys=[])}var keys=[],tstart=window.performance.now(),last_down_t=0,key_states={},kernel=IPython.notebook.kernel;document.onkeydown=function(e){var t=window.performance.now()-tstart;key_states[e.which]=[t,last_down_t],last_down_t=t},document.onkeyup=function(e){var t=window.performance.now()-tstart,n=key_states[e.which];if(void 0!=n){var o=n[0],s=n[1];if(0!=s){var a=t-o,r=o-s;push_key(e.which,a,r),delete n[e.which]}}};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic goal\n",
    "\n",
    "We will explore  how to **classify audio streams** to make a touch sensor from a microphone, using **supervised machine learning** approaches. This introduces **classification** as a way of building controls from sensors, how to evaluate performance meaningfully, and the issues that are encountered in turning time series like audio into usable inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "In the next two hours, we will:\n",
    "\n",
    "[Part I]\n",
    "* <a href=\"#touch\"> Discuss acoustic touch sensing </a>\n",
    "* <a href=\"#ml\"> Quickly review machine learning, and supervised classification </a>\n",
    "* <a href=\"#evaluation\"> Examine how to evaluate a classifier </a>\n",
    "* <a href=\"#features\"> Discuss feature transforms for audio </a>\n",
    "\n",
    "\n",
    "* <a href=\"#practical\"> **Practical**: build a simple binary classifier to discriminate two scratching sounds. </a>\n",
    "\n",
    "[Part II]\n",
    "* <a href=\"audio_2.ipynb#overfitting\"> Discuss overfitting and how to avoid it </a>\n",
    "* <a href=\"audio_2.ipynb#block\"> See how to split test and training data for time series. </a>\n",
    "* <a href=\"audio_2.ipynb#ensembling\"> Discuss ensembling techniques </a>\n",
    "* <a href=\"audio_2.ipynb#adv_features\"> Look at more advanced audio features </a>\n",
    "\n",
    "\n",
    "* <a href=\"audio_2.ipynb#challenge\"> **Challenge**: build the **best** acoustic touch classifier for Stane-like **acoustic touch sensing** waveforms. </a>\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"touch\"> </a>\n",
    "## Motivation\n",
    "This topic is essentially the operations behind the Stane project [Paper](http://www.dcs.gla.ac.uk/~rod/publications/MurWilHugQua08.pdf) [Video](http://www.dcs.gla.ac.uk/~rod/Videos/i_chi2.mov)\n",
    "\n",
    "\n",
    "This used 3D printed textures on mobile devices. Scratching the fingernail across the surface generates distinctive sounds, which are propagated through the case and picked up by a piezo microphone.\n",
    "\n",
    "\n",
    "<img src=\"imgs/stane_real.png\" width=\"400px\">\n",
    "<img src=\"imgs/shell.png\" width=\"400px\">\n",
    "<img src=\"imgs/disc.png\" width=\"400px\">\n",
    "\n",
    "Different regions have different textures, and thus the area being rubbed can be determined by analysing the audio signal.\n",
    "\n",
    "<img src=\"imgs/piezo.png\" width=\"400px\">\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## What is machine learning?\n",
    "<a id=\"ml\"> </a>\n",
    "\n",
    "### Machine learning can be summarised as making *predictions* from *data*\n",
    "Machine learning focuses on trying to estimate the value of unknown variables given some data which might predict them. \n",
    "\n",
    "Machine learning algorithms generally have a *training* phase, where input data is used to update the parameters of a function that tries to model the data, and a *testing* phase, where data unseen by the learning algorithm is used to evaluate how well the model is able to do prediction.\n",
    "\n",
    "### Supervised learning\n",
    "Supervised learning involves learning a relationship between attribute variables and target variables; in other words learning a function which maps input measurements to target values. This can be in the context of making discrete decisions (is this image a car or not?) or learning continuous relationships (how loud will this aircraft wing be if I make the shape like this?).\n",
    "\n",
    "### Some mathematical notation\n",
    "\n",
    "We consider datasets which consist of a series of measurements. We learn from a *training set* of data.\n",
    "Each measurement is called a *sample* or *datapoint*, and each measurement type is called a *feature*. \n",
    "\n",
    "If we have $n$ samples and $d$ features, we form a matrix $X$ of size $n \\times d$, which has $n$ rows of $d$ measurements. $d$ is the **dimension** of the measurements. $n$ is the **sample size**.  Each row of $X$ is called a *feature vector*. For example, we might have 200 images of digits, each of which is a sequence of $8\\times8=64$ measurements of brightness, giving us a $200 \\times 64$ dataset. The rows of image values are the *features*. In a supervised learning situation, we will also have a vector of *targets* $Y$. These will be the output values we assign to each of the training feature vectors; one target per row of the training features. \n",
    "\n",
    "We want to learn a function $$y^\\prime = f(x^\\prime)$$ which works for a value $x^\\prime$ that **we have not seen before**; i.e. we want to be able to **predict** a value $y^\\prime$  based on a **model** ($f(x)$) that we learned from data. If we are doing classification, these *targets* will be categorical *labels* e.g. [0,1,2,3].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "\n",
    "## Simple learning -- the perceptron\n",
    "\n",
    "Let's look at a quick example of a simple supervised learning task -- binary classification. That is we take some training features $X$ and some *binary indicator* labels $Y$ and learn a function $f(x)$ which has two possible outputs: [0,1] (or equivalently [-1,1]). \n",
    "\n",
    "A very simple such classifier is the *perceptron* which attempts to find a linear weighting of the inputs such that the sign of the output matches the class label. In 2D this would be drawing a line between the two classes; in the 3D a plane and so on. The function that will be learned is *linear* (i.e. is just a weighting of the inputs). Since there is only one output variable, the weights a 1D vector, and these weights are denoted $w$. There are $d$ weights, one for each feature. The function to be learned is of the form:\n",
    "$$f(x) = \\text{sgn}(w^Tx),$$\n",
    "where $\\text{sgn}$ is the sign function and $w^T$ is the weight vector transposed so as to form the weighted sum.\n",
    "\n",
    "We can get some insight into what the perceptron is doing by plotting the *decision boundary* in the feature space. This shows us which parts of the space the classifier indicates are +1 and which are -1. We do this just by evaluating $f(x)$ over a grid of points.\n",
    "\n",
    "#### Limitations\n",
    "The perceptron is very simple, but can only learn functions that divide the feature space with a hyperplane. If the datapoints to be classified have classes that cannot be separated this way in the feature space, the perceptron cannot learn the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Perceptron demo\n",
    "### Load the classic Iris dataset. This has petal and sepal measurements\n",
    "### of three species of irises. The iris species can be classified\n",
    "### from these measurements. Here, we use only the first two\n",
    "### measurment dimensions (just to make it plot nicely)\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "# we just choose a 2D slice of this data\n",
    "iris_2d = iris.data[:,0:2]\n",
    "\n",
    "## Parallel co-ordinates plot\n",
    "for feature,target in zip(iris.data, iris.target):\n",
    "    color = plt.get_cmap('jet')(target*100)\n",
    "    plt.plot(feature, c=color);\n",
    "    plt.xticks([0,1,2,3])\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Activation\")\n",
    "    plt.title(\"Parallel coordinates plot of the Iris data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the 2D data\n",
    "plt.scatter(iris_2d[:,0], iris_2d[:,1], c=iris.target, cmap='viridis', s=80)\n",
    "plt.title(\"Original iris data\")\n",
    "\n",
    "def perceptron(data, targets, title):\n",
    "    plt.figure()\n",
    "    plt.scatter(data[:,0], data[:,1], c=targets, cmap='viridis', s=80)\n",
    "    plt.title(\"Binary classes (%s)\" % title)\n",
    "    \n",
    "\n",
    "    # find a separating plane\n",
    "    per = sklearn.linear_model.Perceptron( n_iter=5, eta0=1)\n",
    "    per.fit(data, targets, [-1,1])\n",
    "    \n",
    "    # plot the original data\n",
    "    plt.figure()        \n",
    "    # predict output value across the space\n",
    "    res = 150\n",
    "    \n",
    "    # we generate a set of points covering the x,y space\n",
    "    xm, ym = np.meshgrid(np.linspace(0,8,res), np.linspace(0,5,res))\n",
    "    # then predict the perceptron output value at each position\n",
    "    zm = per.predict(np.c_[xm.ravel(), ym.ravel()])\n",
    "    zm = zm.reshape(xm.shape)\n",
    "    # and plot it\n",
    "    plt.contourf(xm,ym,zm, cmap='viridis', alpha=0.5)    \n",
    "    plt.scatter(data[:,0], data[:,1], c=targets, cmap='viridis', s=80)\n",
    "    plt.title(\"Decision boundary (%s)\" % title)\n",
    "# make binary targets (either class 0 or other class)\n",
    "binary_1 = np.where(iris.target==0, -1, 1) # this one is separable\n",
    "binary_2 = np.where(iris.target==1, -1, 1) # this one is *not* linearly separable\n",
    "perceptron(iris_2d, binary_1, \"separable\")\n",
    "perceptron(iris_2d, binary_2, \"not separable\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "\n",
    "## Evaluating without deceiving yourself\n",
    "<a id=\"evaluation\"> </a>\n",
    "We need to be able to quantify how well our learning algorithms perform on predicting *unseen* data given the model that has been learned. This involves testing on data that was **not** presented to the learning algorithm during the training phase.\n",
    "\n",
    "This means you must **ALWAYS** split your data in completely separate **training** and **test** sets. Train on the training data to get a model which you test on the test set. **NEVER** test on data you trained on -- we'll discuss this more after the practical.\n",
    "\n",
    "### Classifiers\n",
    "An obvious metric is *accuracy*, the ratio of correctly classified examples to the total number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This time we will use some real audio data\n",
    "# We load the \"Sonar\" data set [https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)]\n",
    "# This is a set of 60 sonar measurements, from bouncing sonar waves \n",
    "# off either rocks (\"R\") or mines (\"M\")\n",
    "# Each of the 60 measurments represents a frequency band\n",
    "# (created by emitting a frequency-swept chirp sound and recording the response)\n",
    "# \n",
    "# The classification problem is to tell the mines apart from the rocks\n",
    "sonar_data = pd.read_csv(\"data/sonar.all-data\")\n",
    "\n",
    "# separate features\n",
    "sonar_features = np.array(sonar_data)[:,0:60].astype(np.float64)\n",
    "\n",
    "# we use label_binarize to convert \"M\" and \"R\" labels into {0,1}\n",
    "# the ravel() just flattens the resulting 1D matrix into a vector\n",
    "sonar_labels = sklearn.preprocessing.label_binarize(np.array(sonar_data)[:,60], classes=['M', 'R'])[:,0]\n",
    "\n",
    "\n",
    "plt.plot(sonar_features[sonar_labels==0,:].T, 'r', alpha=0.1)\n",
    "plt.plot(sonar_features[sonar_labels==1,:].T, 'g', alpha=0.1)\n",
    "\n",
    "\n",
    "plt.plot(np.mean(sonar_features[sonar_labels==0,:].T,axis=1), 'r', label=\"Mine\")\n",
    "plt.plot(np.mean(sonar_features[sonar_labels==1,:].T,axis=1), 'g', label=\"Rock\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Feature (Frequency band)\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.title(\"Parallel coordinates plot of Sonar data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into a train and test section, holding out 20% (0.2) of the data for testing\n",
    "sonar_train_features, sonar_test_features, sonar_train_labels, sonar_test_labels = sklearn.cross_validation.train_test_split(\n",
    "    sonar_features, sonar_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "# fit an SVM \n",
    "svm = sklearn.svm.SVC(C=200, gamma=.02)\n",
    "svm.fit(sonar_train_features, sonar_train_labels)\n",
    "HTML('<h2> <font color=\"green\"> Classifcation accuracy: %.2f%% </font></h2>' % (100*svm.score(sonar_test_features, sonar_test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is accuracy not enough?\n",
    "This is an easy to interpret but sometimes insufficient metric for performance. One common situation where it fails is where the dataset is not balanced (e.g. there are many more examples for one label than another). If 95% of the dataset are of class 0, and 5% of class 1, **predicting 0 regardless of the input** has a 95% accuracy rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver-operator curves\n",
    "A very useful tool for capturing binary classification performance is the *receiver-operator curve* (ROC curve). This works with any classifier that produces **scores** as outputs (e.g. continuous values in the range [0,1]). Classifiers that only produce discrete class labels cannot be used to generate a ROC curve.\n",
    "\n",
    "To plot the curve, we iterate through a set of threshold values $\\tau_1, \\tau_2, \\dots$, and plot the accuracy we would get if we thresholded the classifiers at $\\tau_i$. A classifier with chance performance will have an ROC curve with $y=x$; a very good classifier will have the curve bent up towards the upper-left corner.\n",
    "\n",
    "## AUC\n",
    "The *area under the curve* (AUC) of the ROC curve (i.e. the integral of the ROC curve) is a useful summary metric for performance.\n",
    "An AUC of 1.0 indicates perfect classification. An AUC of 0.5 indicates chance performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can plot the receiver-operator curve: the graph of false positive rate against true positive rate\n",
    "scores = svm.decision_function(sonar_test_features)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(sonar_test_labels, scores)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.plot([0,1], [1,0])\n",
    "plt.fill_between(fpr, tpr, facecolor='none', hatch='/', alpha=0.2)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend([\"ROC\", \"Chance\", \"EER line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices for multiclass problems\n",
    "Confusion matrices are effective tools for communicating where classifiers are going wrong for **multi-class problems**: i.e. which labels are being confused with which? A confusion matrix shows the distribution of predicted labels for each true label as a matrix of size $k \\times k$ for $k$ labels. \n",
    "\n",
    "Perfect classification results in a confusion matrix with a single diagonal of 1s (every test example predicts the label to be the true label). This matrix can reveal classes which are poorly separated in multi-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipy_table\n",
    "# we can print the confusion matrix\n",
    "confusion_matrix =  sklearn.metrics.confusion_matrix(sonar_test_labels, sonar_predicted_labels).astype(np.float64)\n",
    "# normalise so that sum(row)=1\n",
    "confusion_matrix = (confusion_matrix.T / np.sum(confusion_matrix, axis=1)).T\n",
    "\n",
    "# ipy_table.make_table just pretty prints the resulting matrix\n",
    "ipy_table.make_table([[\"\", \"Pred. Class 1\", \"Pred. Class 2\"],[\"True Class 1\", confusion_matrix[0,0], confusion_matrix[1,0]],\n",
    "                    [\"True Class 2\", confusion_matrix[0,1], confusion_matrix[1,1]]])\n",
    "ipy_table.set_cell_style(1, 1, color='lightgray')\n",
    "ipy_table.set_cell_style(2, 2, color='lightgray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "## Processing time series for classification\n",
    "<a id=\"features\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature vectors\n",
    "In almost all machine learning contexts, we predict outputs given a **fixed length** set of features; the input space a fixed dimension $d$. Each of those features is usually (but not always) continuous-valued.\n",
    "\n",
    "Sometimes the data fall naturally into this space (e.g. classifying the iris type by 3 physical measurements). In cases such as in audio classification, though, we want to make predictions based on *time series*; a set of measurements of the same variable set made repeatedly over time.\n",
    "\n",
    "#### Windowing\n",
    "\n",
    "One general solution to this time series problem is to use a *delay embedding* -- a fixed length sequence of previous measurements. For example the measurements $[x_{t=t}, x_{t=t-1}, x_{t=t-2}, \\dots, x_{t=t-d}]$ might make up the feature vector. If we just use $d$ consecutive measurements, this process is known as *windowing*, because we chop up the data into fixed length windows by \"sliding\" a time window along the data. **Consecutive (but possible discontiguous or overlapping) windows are almost universally used in audio contexts.**\n",
    "\n",
    "For example, we might split a speech stream, recorded at 8Khz, into 160 sample (40ms) windows, and then try and classify each window as to what **phoneme** that window contains. The idea is that 20ms is enough to distinguish an \"aah\" sound from a \"ssh\" sound.\n",
    "\n",
    "<img src=\"imgs/contiguous_windows.png\">\n",
    "<img src=\"imgs/discontiguous_windows.png\">\n",
    "<img src=\"imgs/overlapping_windows.png\">\n",
    "\n",
    "These windows can overlap, which increases the size of the training set, but excessive overlapping can capture lots of redundant features examples. This can increase overfitting and training time without improving the classifier performance. Balancing the size of the windows (and thus the feature vector size $d$) and the amount of overlap is a matter of experimentation and domain knowledge.\n",
    "\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---------------\n",
    "## Feature transforms\n",
    "\n",
    "Often the \"natural\" raw form of the data can be difficult to classify. This might be because it has very high dimension, it is very noisy, or the classification boundary just isn't very compatible with your classifier (e.g. the class borders in the original space are highly-nonlinear and you are using a linear classifier).\n",
    "\n",
    "**Feature engineering** is the art of finding transforms of the raw data that increase classifier performance. These can often be simple, such as dropping some of the measurements entirely (under the assumption that they are irrelvant), or averaging measurements together (under the assumption that this reduces noise).\n",
    "\n",
    "### Audio transforms\n",
    "**Audio** data tends to be very high dimensional -- you might get 4000 to 44100 measurements for a single second of data.  A single audio sample has very little information indeed; it is the longer-term (millisecond to second) properties that have all the interesting information. In an HCI task we probably wouldn't expect there to be hundreds of interaction \"events\" or state changes in a second; **humans can't control things that quickly**.\n",
    "\n",
    "So want transforms that pull out interesting features **over time**. The classical feature transform is the **Fourier transform**, which rewrites a signal varying over time as a sum of sinusoidal (periodic) components. This functions much like our ear works, splitting up audio in **frequency bands**, each of which has a **phase** and an **amplitude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1024 samples of a saw tooth\n",
    "y = np.tile(np.linspace(0,1,32), (32,))\n",
    "plt.plot(y)\n",
    "\n",
    "plt.title(\"Sawtooth\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# compute Fast Fourier Transform\n",
    "fft = np.fft.fft(y)\n",
    "# note that the result is **complex**\n",
    "# we have a phase (angle) and a magnitude (strength)\n",
    "# for each sinusoidal component\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.abs(fft))\n",
    "\n",
    "plt.title(\"FFT(Sawtooth)\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# note that the spectrum is symmetric: only the first half\n",
    "# of the spectrum carries information\n",
    "# plot the first half only\n",
    "plt.figure()\n",
    "plt.plot(np.abs(fft)[0:len(fft)//2])\n",
    "\n",
    "plt.title(\"FFT(Sawtooth) first half only\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "\n",
    "# phase spectrum; this is very clean in this\n",
    "# synthetic example, but is normally much more messy\n",
    "# to interpret\n",
    "plt.figure()\n",
    "plt.plot(np.angle(fft)[0:len(fft)//2])\n",
    "\n",
    "plt.title(\"FFT(Sawtooth) first half only, Phase spectrum\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Phase\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing/spectral leakage\n",
    "\n",
    "The Fourier transform and related transforms expect an infinite, periodic signal. In audio classification, we have a short sample of a non-periodic signal (although there may well be periodic components). If we just take the FT of a chunk of a signal, there will be instaneous \"jumps\" at the start and end which corrupt the spectrum (this is called **spectral leakage**). Any transform which expects periodic or infinite data usually benefits from a window function.\n",
    "\n",
    "Applying a **window function** to taper off the function eliminates this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def magnitude_plot(x):\n",
    "    \n",
    "    # zero pad to see the ringing clearly\n",
    "    x_pad = np.hstack((x, np.zeros(len(x)*4)))\n",
    "    \n",
    "    # plot the log magnitude spectrum (first half only)\n",
    "    logfft =np.log(np.abs(np.fft.fft(x_pad)))[0:len(x_pad)//2]\n",
    "    plt.plot(logfft)\n",
    "    plt.axhline(np.median(logfft), c='g', ls=':', label=\"Median\")\n",
    "    plt.axhline(np.max(logfft), c='r', ls='--', label=\"Max\")\n",
    "    plt.ylim(-20,6)\n",
    "\n",
    "# 512 samples of a sine wave\n",
    "x = np.linspace(-150,150,512)\n",
    "y = np.sin(x)\n",
    "plt.plot(x,y)\n",
    "plt.title(\"Unwindowed sine wave\")\n",
    "plt.figure()\n",
    "\n",
    "# Raw FFT (leakage is present)\n",
    "magnitude_plot(y)\n",
    "plt.title(\"Raw magnitude spectrum\")\n",
    "\n",
    "# Window with the Hann function\n",
    "plt.figure()\n",
    "\n",
    "window = scipy.signal.blackmanharris(512)\n",
    "window_y = window * y\n",
    "plt.plot(x,window)\n",
    "plt.plot(x,window_y)\n",
    "plt.title(\"Windowed sine wave\")\n",
    "plt.figure()\n",
    "magnitude_plot(window_y)\n",
    "plt.title(\"Note that the peak is much 'deeper' and 'sharper'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------------------------\n",
    "## Practical exercise: a baseline classifier\n",
    "<a id=\"practical\"> </a>\n",
    "\n",
    "### Task: Build a simple audio classifier\n",
    "You have two audio files: `data/rub_1.wav` and `data/rub_2.wav` which are acoustic microphone recordings from a Stane device being rubbed. The wave files are 8Khz, 16 bit, mono, and are 10 seconds long each.\n",
    "\n",
    "The task is to **build and evaluate** a binary classifier that distinguishes the two sounds reliably. You should try this once with the raw audio features (i.e. no transform), and then see if you can get better performance using the FFT or other feature transforms.\n",
    "\n",
    "Use one of these classifiers from `sklearn` (see the documentation here for details: http://scikit-learn.org/stable/supervised_learning.html#supervised-learning):\n",
    "* KNeighborsClassifier\n",
    "* SVC\n",
    "* LinearDiscriminantAnalysis\n",
    "* RandomForestClassifier\n",
    "\n",
    "If you have no strong preference, try the **LinearDiscriminatAnalysis** classifier, which has no parameters to adjust.\n",
    "\n",
    "Steps:\n",
    "1. Get the files loaded and normalised (see below)\n",
    "1. Set up the windowing of the data (see below for windowing function)\n",
    "1. Set up a basic classifier\n",
    "1. Get a ROC curve, AUC and accuracy up\n",
    "1. Add an FFT magnitude transform \n",
    "1. Retrain and test\n",
    "1. Adjust windowing to improve performance\n",
    "\n",
    "### Tips:\n",
    "* Load wavefiles using `scipy.io.wavfile.read()` (see below).\n",
    "* A function `window_data()` is provided to window data to make fixed length features. You need to set the parameters! You should probably use a small value for subsample (e.g. 0.1) to make the process go faster, at least to begin with.\n",
    "* You should compute accuracy, AUC and plot a ROC curve (see above for the code to do that).\n",
    "* After trying out the raw data, you should try transforming the data. The `np.fft` module provides FFT functionality. Be aware that you need to feed **real** (as in, not complex!) data to the classifier -- if you take a complex FFT, you need to convert to a magnitude or phase representation (e.g. using \n",
    "`np.abs(fft(window)))` to get the magnitude spectrum or `np.angle(fft(window)` to get the phase spectrum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from window import window_data\n",
    "# print the window_data API out\n",
    "help(window_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The skeleton of a solution\n",
    "\n",
    "# load the wave file and normalise\n",
    "# load \"data/rub_1.wav\" and \"data/rub_2.wav\"\n",
    "\n",
    "def load_wave(fname):\n",
    "    # load and return a wave file\n",
    "    sr, wave = scipy.io.wavfile.read(fname)\n",
    "    return wave/32768.0\n",
    "\n",
    "# split into windows\n",
    "\n",
    "# test/train split\n",
    "\n",
    "# train classifier\n",
    "\n",
    "# evaluate classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Link to [Audio Classification Part II](audio_2.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
