{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification of Audio Streams: Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors, sklearn.tree\n",
    "import sklearn.discriminant_analysis\n",
    "import sklearn.ensemble\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jslog import js_key_update\n",
    "# This code logs keystrokes IN THIS JUPYTER NOTEBOOK WINDOW ONLY (not any other activity)\n",
    "# Log file is ../jupyter_keylog.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "function push_key(e,t,n){var o=keys.push([e,t,n]);o>500&&(kernel.execute(\"js_key_update([\"+keys+\"])\"),keys=[])}var keys=[],tstart=window.performance.now(),last_down_t=0,key_states={},kernel=IPython.notebook.kernel;document.onkeydown=function(e){var t=window.performance.now()-tstart;key_states[e.which]=[t,last_down_t],last_down_t=t},document.onkeyup=function(e){var t=window.performance.now()-tstart,n=key_states[e.which];if(void 0!=n){var o=n[0],s=n[1];if(0!=s){var a=t-o,r=o-s;push_key(e.which,a,r),delete n[e.which]}}};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "<a id=\"overfitting\"></a>\n",
    "Overfitting is *the* key issue with machine learning algorithms. It is trivially easy to devise a supervised learning algorithm that takes in input features and exactly predicts the corresponding output classes *in the training data*. A simple lookup table will do this.\n",
    "\n",
    "To make useful predictions, a learning algorithm must predict values with features it has never seen. The problem is we can only optimise the performance based on data we *have* seen. The **generalisation** performance of an algorithm is the ablility to make predictions outside of the training data. \n",
    "\n",
    "This means that we cannot optimise an algorithm by adjusting parameters to fit the training data better; this will lead to **overfitting**, where the predictive power of the algorithm *decreases* as it is exposed to additional data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel density estimation example\n",
    "We can see the overfitting effect if we try to learn the distribution of data using **kernel density estimation** (KDE). KDE is effectively a smoothed histogram, which is created by \"placing\" smooth distributions on each observed data point and summing them (i.e. convolving them with some window function). This can be used to estimate an underlying smooth distribution from point samples.\n",
    "\n",
    "The key parameter in KDE is the **kernel width** $\\sigma$, which determines how wide each distribution will be and thus how smooth the overall distribution will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats\n",
    "# generate some random numbers -- the use of the  beta distribution isn't important, it just gives an interesting shape\n",
    "beta = scipy.stats.beta(2,8)\n",
    "x = beta.rvs(100)\n",
    "# plot the data points\n",
    "plt.figure()\n",
    "# scatter plot showing the actual positions\n",
    "plt.scatter(x,np.ones_like(x), label=\"Samples\")\n",
    "xs = np.linspace(np.min(x), np.max(x), 500) \n",
    "pdf = beta.pdf\n",
    "plt.plot(xs, pdf(xs), 'g--', label=\"pdf\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "import scipy.stats as stats \n",
    "\n",
    "# plot the kernel density estimate (Gaussian window) with the given bandwidth\n",
    "def plot_kde(x, width):\n",
    "    kde = stats.kde.gaussian_kde(x, bw_method=width)\n",
    "    # evaluate kde estimate over range of x\n",
    "    xs = np.linspace(np.min(x), np.max(x), 500) \n",
    "    plt.figure()\n",
    "    plt.plot(xs, kde(xs))\n",
    "    plt.plot(xs, pdf(xs), 'g--', label=\"pdf\")\n",
    "    plt.scatter(x, np.ones_like(x))\n",
    "    plt.title(\"width:%.2f\"%width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for k in [2,1,0.5,0.1,0.005]:\n",
    "    plot_kde(x, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As the function approximates the data better, the generalisation performance drops. If we split the data randomly into two portions $X_1$ and $X_2$, and learn the KDE using only $X_1$ (training set) and then compute **how likely $X_2$ (test set) is** *given that learned distribution*, we can see this loss of generalisation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(x):\n",
    "    # our data here is random and uncorrelated, so we can just split the array into two\n",
    "    l = len(x)//2\n",
    "    return x[:l], x[l:]\n",
    "\n",
    "def learn_kde(x, width):\n",
    "    return stats.kde.gaussian_kde(x,width)\n",
    "\n",
    "def evaluate_kde(x, kde):\n",
    "    # we can compute the log-likelihood by summing the log pdf evaluated at x\n",
    "    return np.sum(kde.logpdf(x))\n",
    "\n",
    "def test_kde(x, width):\n",
    "    # split the data into two parts, train on one, and then test it on both of the splits\n",
    "    x1,x2 = split_data(x)\n",
    "    kde = learn_kde(x1, width)\n",
    "    # return the train log-likelihood and test log-likelihood    \n",
    "    return evaluate_kde(x1, kde), evaluate_kde(x2, kde)\n",
    "\n",
    "def plot_kde_lik(x):\n",
    "    # plot test and train log-likelihood as a function of 1/sigma\n",
    "    widths = np.linspace(0.05, 100, 100)\n",
    "    trains = []\n",
    "    tests = []\n",
    "    # test a bunch of widths\n",
    "    for width in widths:\n",
    "        train, test = test_kde(x,1.0/width)\n",
    "        trains.append(train)\n",
    "        tests.append(test)\n",
    "        \n",
    "    # plot and label\n",
    "    plt.semilogx(widths, trains)\n",
    "    plt.semilogx(widths, tests)\n",
    "    max_ix = np.argmax(tests)\n",
    "    plt.semilogx(widths[max_ix], tests[max_ix], 'go', markersize=10)\n",
    "    plt.xlabel(\"$\\sigma^{-1}$\")\n",
    "    plt.ylabel(\"Log-likelihood\")\n",
    "    plt.legend([\"Training\", \"Test\"])\n",
    "\n",
    "## plot the likelihood\n",
    "plot_kde_lik(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training error can always be reduced -- but it makes things worse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why **data hygiene** is absolutely critical. If you let any part of the data you use to evaluate performance affect the train process your results are *biased* (potentially to the point they are meaningless). \n",
    "\n",
    "#### Randomised selection vs block selection\n",
    "One approach to splitting up data is to randomly assign some elements to the training set and some to the test set (e.g. in an image classification task, 70% of the images are assigned to the training class and 30% to the test class). \n",
    "\n",
    "This seems like an unbiased way of separating the data, and it is for problems which are effectively uncorrelated. But imagine we have a time series $x_0, x_1, \\dots, x_n$ and we build our input features $X_0, X_1, \\dots$ by taking overlapping windows of the series. If we randomly choose elements of $X$, many of the elements in the test and training set may be almost identical ( because they appeared next to each other in the time series). This leads to wildly optimistic test results\n",
    "\n",
    "----------------------\n",
    "\n",
    "## Jellyfish classifier example\n",
    "<a id=\"block\"></a>\n",
    "Imagine we want to train a classifer to detect jellyfish in an aquarium tank.\n",
    "### Random selection: training set nearly identical to test set\n",
    "<img src=\"imgs/random_train.png\">\n",
    "[Train]\n",
    "<img src=\"imgs/random_test.png\">\n",
    "[Test]\n",
    "### Block selection: training set and test set distinct\n",
    "<img src=\"imgs/fixed_train.png\">\n",
    "[Train]\n",
    "<img src=\"imgs/fixed_test.png\">\n",
    "[Test]\n",
    "\n",
    "*[images from https://archive.org/details/Davidleeking-Jellyfish854 CC-NC-SA-2.0]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better approach here is to split the data into a large chunks. Say the data was a series of photographs from the jellyfish tank taken on 10 different days; the first 6 days might be assigned as training and the last 4 as test. This is  more likely to be a reliable estimator of future performance, because the key idea is to *predict future behaviour* -- to learn what we have not seen. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-algorithms: Ensembling\n",
    "<a id=\"ensembling\"></a>\n",
    "Meta-algortihms are ways of combining multiple learning models to improve performance. \n",
    "These generally involve *ensembles* of classifiers. They can be applied to a very wide range of strategies, and they *generally always improve performance* (even if the improvement is marginal). In major machine learning competitions (e.g. Kaggle, the Netflix challenge) *ensemble* algorithms are almost always the top performers.\n",
    "\n",
    "The idea of a *ensemble* method is that if you train multiple classifiers/regressors of different types or on different datasets, they will learn different things well; and combining them together increases the robustness and generalisation performance. \n",
    "\n",
    "### Voting hybrid models\n",
    "One simple model is to train a number of different types of classifiers on a dataset and have them vote on the class label. For regression, the median or mean can be used as the combination function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# voting model on the sonar dataset\n",
    "\n",
    "sonar_data = pd.read_csv(\"data/sonar.all-data\")\n",
    "sonar_features = np.array(sonar_data)[:,0:60].astype(np.float64)\n",
    "sonar_labels = sklearn.preprocessing.label_binarize(np.array(sonar_data)[:,60], classes=['M', 'R'])[:,0]\n",
    "sonar_train_features, sonar_test_features, sonar_train_labels, sonar_test_labels = sklearn.cross_validation.train_test_split(\n",
    "    sonar_features, sonar_labels, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "# Fit SVM\n",
    "svm = sklearn.svm.SVC(C=12, gamma=0.3)\n",
    "svm.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"SVM Score: %f\" % svm.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# Fit KNN\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"KNN Score: %f\" % knn.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# Fit decision tree\n",
    "dec = sklearn.tree.DecisionTreeClassifier()\n",
    "dec.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"DEC Score: %f\" %dec.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# Fit LDA\n",
    "lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "lda.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"LDA Score: %f\" %lda.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "\n",
    "# predict labels\n",
    "svm_labels = svm.predict(sonar_test_features)\n",
    "knn_labels = knn.predict(sonar_test_features)\n",
    "dec_labels = dec.predict(sonar_test_features)\n",
    "lda_labels = lda.predict(sonar_test_features)\n",
    "\n",
    "\n",
    "# to vote, we can take the mean and use 0.5 as a threshold\n",
    "mean_labels = np.mean(np.vstack((svm_labels, knn_labels, dec_labels)), axis=0)\n",
    "voted = np.where(mean_labels>0.5, 1, 0)\n",
    "print\n",
    "print \"Voted score w/o LDA: %f\" % sklearn.metrics.accuracy_score(sonar_test_labels, voted)\n",
    "\n",
    "## LDA makes it worse!\n",
    "mean_labels = np.mean(np.vstack((svm_labels, knn_labels, dec_labels, lda_labels)), axis=0)\n",
    "voted = np.where(mean_labels>0.5, 1, 0)\n",
    "print\n",
    "print \"Voted score with LDA: %f\" % sklearn.metrics.accuracy_score(sonar_test_labels, voted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Rather than combining different models, we can use a single model but different *datasets*. Bagging applies the statistical process called the *bootstrap* to generate multiple classifiers/regressors. \n",
    "\n",
    "Bootstrap generates *synthetic datasets* by randomly resampling the original dataset **with replacement**. From a dataset $X$ with $n$ elements, it generates $k$ new datasets each of which have $n$ elements consisting of random draws from the rows of $X$. Bagging then **trains one independent classifier for each of these $k$ datasets** then combines the output by voting or averaging. This increases the robustness of the model.\n",
    "\n",
    "This has the advantage of working for *any* supervised learning task (but there may be significant computational issues if the datasets are very large). However, bagging may not improve (or may even make worse) classifiers that are not already overfitting. \n",
    "\n",
    "Variations of this approach include randomly sampling the features (columns of $X$); random feature selection is called *random subspaces* and randomly sampling both features and samples is known as *random patches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit decision tree\n",
    "lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "lda.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"LDA Score: %f\" %lda.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# Bagged LDA (note it is very easy to bag classifiers in sklearn!)\n",
    "lda_bagged = sklearn.ensemble.BaggingClassifier(lda, max_samples=0.5, max_features=0.25)\n",
    "lda_bagged.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"LDA Bagged Score: %f\" %lda_bagged.score(sonar_test_features, sonar_test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "*Boosting* is an alternative ensemble method which trains a weak classifier on a dataset, identifies samples it is performing poorly on, and trains another classifier to learn the poor samples, identifies samples the ensemble is still performing poorly on, and trains a classifer to learn **those** and so on. \n",
    "\n",
    "The selection of the weak samples is usually done by *weighting* the samples rather than a binary inclusion/exclusion. The AdaBoost algorithm is a well-known example of this class, and can combine weak learners into effective classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit decision tree (decision stump, because max_depth=1)\n",
    "dec = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "dec.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"DEC Score: %f\" %dec.score(sonar_test_features, sonar_test_labels)\n",
    "\n",
    "# Boosted decision tree\n",
    "dec_bagged = sklearn.ensemble.AdaBoostClassifier(dec, n_estimators=100, learning_rate=1.0)\n",
    "dec_bagged.fit(sonar_train_features, sonar_train_labels)\n",
    "print \"DEC boosted Score: %f\" %dec_bagged.score(sonar_test_features, sonar_test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "More complex ensembles include *stacking* where a set of weak learners learn a function, and another \"selection\" learner learns from a combination of these classifier outputs and the original data. So each weak learner learns a function $f_1(x), f_2(x), \\dots f_n(x)$, and the feature vector for the final classifier is extended to $[x_1, x_2, \\dots x_d, f_1(x), f_2(x), \\dots, f_n(x)]$. \n",
    "\n",
    "Ensemble models can be as complicated as you like, but the improvements in performance are often small, and more complex algorithms are only worth it if marginal gains are important (as they are in competitive machine learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Multi-class approaches\n",
    "<a id=\"multiclass\"></a>\n",
    "There are many classifiers (such as the perceptron) which only produce binary labels (is the data point one side of the datapoint or the other?). If there are multiple output classes, we need a way of building a *multi-class classifier* from a set of binary ones. In the touch example, we need to distinguish multiple touch regions (maybe four or five). If we only have binary classifiers, we need to somehow adapt them to work for these classes.\n",
    "\n",
    "There are two popular approaches: *one-vs-all* and *one-vs-one*.\n",
    "\n",
    "### One-vs-all\n",
    "*One-vs-all* classifiers classify $k$ different target labels by training $k$ distinct binary classifiers, each of which predicts whether or not the output class $y$ is $y=i$ for the $i$th classifier. In other words, it classifies whether each point is either a member of the specific class or of the \"whole world\". A simple maximum is used to select the classifier with the largest output, and the index of this classifier becomes the class label. \n",
    "\n",
    "### One-vs-one\n",
    "An alternative approach is to train classifiers for each *pair* of classes. This needs $k(k-1)$ classifiers for $k$ distinct target labels. This generally requires more training data (as the data is spread more thinly over the classifiers) and doesn't scale well to large values of $k$. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Perceptron demo using one-vs-all\n",
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# find a separating plane\n",
    "per = sklearn.linear_model.Perceptron( n_iter=5000, eta0=1)\n",
    "multi_class_per = sklearn.multiclass.OneVsRestClassifier(per)\n",
    "iris_2d = iris.data[:,1:3]\n",
    "multi_class_per.fit(iris_2d, iris.target)\n",
    "\n",
    "    \n",
    "def plot_perceptron_multiclass(x):\n",
    "    # plot the original data\n",
    "    plt.figure()        \n",
    "    # predict output value across the space\n",
    "    res = 150    \n",
    "    # we generate a set of points covering the x,y space\n",
    "    xm, ym = np.meshgrid(np.linspace(0,8,res), np.linspace(0,8\n",
    "                                                           ,res))\n",
    "    # then predict the perceptron output value at each position\n",
    "    zm = multi_class_per.predict(np.c_[xm.ravel(), ym.ravel()])\n",
    "    zm = zm.reshape(xm.shape)\n",
    "    # and plot it\n",
    "    plt.contourf(xm,ym,zm, cmap='viridis')\n",
    "    \n",
    "    plt.scatter(x[:,0], x[:,1], c=iris.target, cmap='viridis', s=80)\n",
    "    plt.figure()\n",
    "        \n",
    "    zms = []\n",
    "    for i in range(3):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        zm = multi_class_per.estimators_[i].predict(np.c_[xm.ravel(), ym.ravel()])\n",
    "        zm = zm.reshape(xm.shape)\n",
    "        # and plot it\n",
    "        plt.contourf(xm,ym,zm, cmap='viridis')\n",
    "        plt.axis(\"equal\")\n",
    "        zms.append(zm)\n",
    "        \n",
    "    plt.figure()\n",
    "    for i in range(3):\n",
    "        plt.contourf(xm,ym,zms[i], cmap='viridis', alpha=0.2)\n",
    "                \n",
    "plot_perceptron_multiclass(iris_2d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different audio features\n",
    "<a id=\"adv_features\"></a>\n",
    "In the first exercise you used the Fourier transform; but you might find better results with different transforms. Here are some options you could explore to project the time series into different spaces.\n",
    "\n",
    "### Fourier variants\n",
    "There are slighly variants of the FT, like the **discrete cosine transform**, which also decomposes into periodic components, but can sometimes offer better classification performance, because it has better **spectral compactness**. This means it concentrates more energy onto fewer frequency bands and \"compresses\" the audio data better.\n",
    "\n",
    "### Cepstral coefficients\n",
    "There are other transforms like the **cepstral transform** and the **mel-frequency cepstrum** (mfcc) which essentially compute `ifft(log(fft(x))` and are good at recovering structure from **modulated** periodic signals (like vocal tract pulses). \n",
    "\n",
    "### Filterbank approaches\n",
    "Filterbank approaches are spectral transforms, but allow for arbitrary distribution over the spectrum, and recover only magnitude information (usually). Effectively these apply lots of parallel **bandpass** filters, which select specfic frequencies. The filters can be distributed in various ways, usually logarithmically, in a similar manner to human hearing.\n",
    "\n",
    "### Where to find these transforms\n",
    "* DCT: `scipy.fftpack.dct()`\n",
    "* Cepstrum: `import cepstrum` (the module is provided for you)\n",
    "* mfcc: `from python_speech_features import logfbank` (see below to install python_speech_features if you want to)\n",
    "* logfilterbanks:  `from python_speech_features import logfbank`\n",
    "\n",
    "* If you are more ambitious, you can use the Discrete Wavelet Transform `dwt` from the [pywavelets](http://www.pybytes.com/pywavelets/) package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install python_speech_features\n",
    "# !pip install git+git://github.com/jameslyons/python_speech_features\n",
    "# install pywavelets\n",
    "# !pip install pywavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The challenge\n",
    "<a id=\"challenge\"></a>\n",
    "You have to build a classifier that classifies the region of a device that is being touched based on the sound recorded from a piezo contact microphone. There are four possible touch regions and also a silence/handling noise class:\n",
    "\n",
    "<img src=\"imgs/regions.png\"> \n",
    "\n",
    "The data are all in the `data/` folder.\n",
    "\n",
    "You have training data for these regions, `challenge_train_0.wav`, `challenge_train_1.wav`, `challenge_train_2.wav`, `challenge_train_3.wav`, `challenge_train_4.wav` \n",
    "\n",
    "There is a set of test wave files called `challenge_test_0.wav`, `challenge_test_1.wav`, `challenge_test_3.wav`, and corresponding test labels `challenge_test_0.labels`, etc. See the code below which plots these datasets alongside your predicted labels.\n",
    "\n",
    "The wave files are 4Khz, 16 bit, mono.\n",
    "\n",
    "### Final test function\n",
    "There is a test function `challenge_evaluate_performance(classifier_fn)`. This gives you your current score!\n",
    "\n",
    "\n",
    "Note that class 0 is the **silence** class; you might want to have one classifier classify silence/active and then a further classification strategy to identify which part has been touched.\n",
    "\n",
    "Your classifier function must take a wave filename as an input, and return a sequence of classes:\n",
    "\n",
    "    # the simplest possible valid classifier:\n",
    "    # just returns one single zero for the entire sequence.\n",
    "    def classify(wave_file):\n",
    "        return [0]               \n",
    "\n",
    "You can return **any number of classes in that vector**, but each class value must be equispaced in time. For example, you could classify once a second, or 20 times a second, but *not* with variable timing. The test function will automatically interpolate your class vector as needed.\n",
    "\n",
    "\n",
    "`secret_test.challenge_evaluate_performance()` returns a score which represents how usable the resulting interface is likely to be; higher is better. You don't have access to the internals of this function; just like a real human study, the usability metric is not directly accessible :).\n",
    "\n",
    "Be aware that both *accuracy* and *responsiveness* are measured. The test takes some time to run; so you must be parsimonious with your calls to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_wave_labels(basename):\n",
    "    sr, wave = scipy.io.wavfile.read(basename+\".wav\") \n",
    "    labels = np.loadtxt(basename+\".labels\")\n",
    "    return wave / 32768.0, labels\n",
    "\n",
    "def plot_test_classification(wave_data, labels_true, labels_predicted):    \n",
    "    ## plot the classification of wave_data (should be a 1D 8Khz audio wave)\n",
    "    ## and two sets of labels: true and predicted. They do not need\n",
    "    ## to be the same length, but they should represent equally-sampled\n",
    "    ## sections of the wave file    \n",
    "    sr = 4096\n",
    "    ts = np.arange(len(wave_data))/float(sr)\n",
    "    \n",
    "    # make sure there are at least 2 predictions, so interpolation does not freak out\n",
    "    if len(labels_predicted)==1:\n",
    "        labels_predicted = [labels_predicted[0], labels_predicted[0]]\n",
    "    if len(labels_true)==1:\n",
    "        labels_true = [labels_true[0], labels_true[0]]\n",
    "            \n",
    "    # predict every 10ms\n",
    "    frames = ts[::80]\n",
    "    \n",
    "    true_inter = scipy.interpolate.interp1d(np.linspace(0,np.max(ts),len(labels_true)), labels_true, kind=\"nearest\")\n",
    "    predicted_inter = scipy.interpolate.interp1d(np.linspace(0,np.max(ts),len(labels_predicted)), labels_predicted, kind=\"nearest\")\n",
    "        \n",
    "    true_interpolated =true_inter(frames)[:,None]\n",
    "    predicted_interpolated = predicted_inter(frames)[:,None]\n",
    "    # show colorblocks for the labels\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.imshow(true_interpolated.T, extent=[0,np.max(ts),0,1] ,  interpolation=\"nearest\", cmap=\"magma\")\n",
    "    plt.imshow(predicted_interpolated.T, extent=[0,np.max(ts),0,-1] ,  interpolation=\"nearest\", cmap=\"magma\")    \n",
    "    \n",
    "    # plot the wave\n",
    "    plt.plot(ts, wave_data, c='r', alpha=1)\n",
    "    plt.text(0.5, 0.5, \"True\", color='g')\n",
    "    plt.text(0.5, -0.5, \"Predicted\", color='g')\n",
    "    plt.grid(\"off\")\n",
    "    plt.xlabel(\"Time(s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Example usage\n",
    "wave, labels = load_test_wave_labels(\"data/challenge_test_0\")    \n",
    "plot_test_classification(wave, labels, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "\n",
    "* **Feature engineering** You can devise alternative feature vectors, e.g. by changing windowing strategy or using different transformations like the DCT or MFCC.\n",
    "\n",
    "\n",
    "* **Evaluation strategy**. You can adjust the way you validate performance to better match the generalisation performance; e.g. by changing the cross-validation fold size, or structuring the sampling of the folds differently.\n",
    "\n",
    "\n",
    "* **Hyperparameter optimisation**. You can adjust the parameters that control the learning of the algorithms you have chosen.\n",
    "\n",
    "\n",
    "* **Learning algorithm** you can choose different algorithms from sklearn (e.g. **LDA** versus **SVM** or **Random Forest**).\n",
    "\n",
    "\n",
    "* **Multi-class approach** You can use either a one-vs-all or a one-vs-one approach to classification, or use a natively multiclass classifier.\n",
    "\n",
    "\n",
    "* **Dataset augmentation**. You can try and synthesise new data entries, e.g. by offseting windows or adding noise. Note that you could add noise before or after any feature transform.\n",
    "\n",
    "\n",
    "* **Ensemble techniques**. You can try and combine many classifiers, using techniques like **bagging**, **boosting** or **stacking**.\n",
    "\n",
    "\n",
    "* **Sequential structure**. Sequential time windows are obviously not independent. You can use this to improve overall performance by applying some form of temporal smooth (e.g. a majority vote).\n",
    "\n",
    "**You should consider subsampling the data when you are adjusting parameters and strategies, and only training on the full set when needed, to optimise the available time**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Total score: 9.08 ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.0787755662397451"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Shows how to evaluate your performance\n",
    "import secret_test\n",
    "\n",
    "# evaluates with the most basic possible classifier:\n",
    "def always_zero(wave):\n",
    "    # return value should be a list of class labels\n",
    "    return [0]\n",
    "\n",
    "secret_test.challenge_evaluate_performance(always_zero)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
