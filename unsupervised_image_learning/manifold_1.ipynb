{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning: Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors\n",
    "import sklearn.manifold, sklearn.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jslog import js_key_update\n",
    "# This code logs keystrokes IN THIS JUPYTER NOTEBOOK WINDOW ONLY (not any other activity)\n",
    "# Log file is ../jupyter_keylog.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "function push_key(e,t,n){var o=keys.push([e,t,n]);o>500&&(kernel.execute(\"js_key_update([\"+keys+\"])\"),keys=[])}var keys=[],tstart=window.performance.now(),last_down_t=0,key_states={},kernel=IPython.notebook.kernel;document.onkeydown=function(e){var t=window.performance.now()-tstart;key_states[e.which]=[t,last_down_t],last_down_t=t},document.onkeyup=function(e){var t=window.performance.now()-tstart,n=key_states[e.which];if(void 0!=n){var o=n[0],s=n[1];if(0!=s){var a=t-o,r=o-s;push_key(e.which,a,r),delete n[e.which]}}};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Purpose\n",
    "In this part, we will explore how **unsupervised learning** can pull out structure from **sensors**. We can use this \"natural\", latent structure to build interfaces without having to predefine our controls.\n",
    "\n",
    "## Outline\n",
    "In the next two hours, we will:\n",
    "\n",
    "[Part I]\n",
    "* <a href=\"#motivation\"> Think about objective methods for designing UI controls </a>\n",
    "* <a href=\"#unsupervised\"> Discuss unsupervised learning </a>\n",
    "* <a href=\"#clustering\"> Explore clustering algorithms < /a>\n",
    "\n",
    "\n",
    "* **Practical**: <a href=\"#practical\"> build a clustering algorithm to cluster images of day and night driving imagery </a>\n",
    "\n",
    "[Part II]\n",
    "* <a href=\"manifold_2.ipynb#manifold\"> Discuss manifold learning </a>\n",
    "* <a href=\"manifold_2.ipynb#som\"> Look at self-organising maps\n",
    "* <a href=\"manifold_2.ipynb#isomap\"> Use ISOMAP to perform manifold learning\n",
    "* <a href=\"manifold_2.ipynb#mapping\"> Look at linking inferred manifolds to UI controls\n",
    "\n",
    "\n",
    "* **Challenge**: <a href=\"manifold_2.ipynb#challenge \">build the **coolest** UI control based on unsupervised learning of webcam imagery </a>\n",
    "\n",
    "\n",
    "### Motivation\n",
    "<a id=\"motivation\"></a>\n",
    "<img src=\"imgs/mainfold_labeled.png\">\n",
    "\n",
    "For many conventional UI sensors, we already have good mappings from **sensor measurements** to **interface actions**. This is largely because the sensors were designed specifically to have electrical outputs which are very close to the intended actions; a traditional mechanical mouse literally emits electrical pulses at a rate proportional to translation.\n",
    "\n",
    "But with optical sensors like a Kinect, or with a high-degree of freedom flexible sensor, or tricky sensors like electromyography (which measures the electrical signals present as muscles contract), these mappings become tricky. Supervised learning lets train a system to recognise patterns in these signals (e.g. to classify poses or gestures). \n",
    "**But what if you don't know what's even feasible or would make a good interface?**\n",
    "\n",
    "### Inherent structures\n",
    "\n",
    "If we take sensor measurements of a person doing \"random stuff\" (see [Rewarding the Original](http://www.dcs.gla.ac.uk/~jhw/motionexplorerdata/) CHI 2012 for ideas on how to make \"random stuff\" a formal process!), we will will end up with a set of feature vectors that were both **performable** and **measurable** (because we know someone did them and a sensor measured them). *You will get a chance to apply the ‘Rewarding the Original’ approach tomorrow in the practicals associated with Rod’s lecture, so if you have time this evening, feel free to have a read of the paper, or watch the video in advance*\n",
    "\n",
    "One way to look at this data is to recover **inherent structure** in these measurements -- are there **regularities** or **stable points** which represent things which might be good controls?\n",
    "\n",
    "\n",
    "<img src=\"imgs/motion_video_frame.png\">\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Unsupervised learning\n",
    "<a id=\"unsupervised\"></a>\n",
    "Unsupervised learning learns \"interesting things\" about the structure of data without any explicit labeling of points. The key idea is that datasets may have a simple underlying or *latent* representation which can be determined simply by looking at the data itself.\n",
    "\n",
    "Two common unsupervised learning tasks are *clustering* and *dimensional reduction*. Clustering can be thought of as the unsupervised analogue of classification -- finding discrete classes in data. Dimensional reduction can be thought of as the analogue of regression -- finding a small set of continuous variables which \"explain\" a higher dimensional set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering tries to find well-seperated (in some sense) **partitions** of a data set. It is essentially a search for natural boundaries in the data. \n",
    "\n",
    "\n",
    "<img src=\"imgs/cluster_img.png\">\n",
    "\n",
    "There are many, *many* clustering approaches. A simple one is *k-means*, which finds clusters via an iterative algortihm. The number of clusters must be chosen in advance. In general, it is hard to estimate the number of clusters, although there are algorithms for estimating this. k-means proceeds by choosing a set of $k$ random points as initial cluster seed points; classifiying each data point according to its nearest seed point; then moving the cluster point towards the mean position of all the data points that belong to it. \n",
    "\n",
    "The k-means algorithm does not guarantee to find the best possible clustering -- it falls into *local minima*. But it often works very well.\n",
    "\n",
    "<img src=\"imgs/cluster_boundary.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "digit_data = digits.data\n",
    "\n",
    "\n",
    "selection = np.random.randint(0,200,(10,))\n",
    "\n",
    "digit_seq = [digit_data[s].reshape(8,8) for s in selection]\n",
    "plt.imshow(np.hstack(digit_seq), cmap=\"gray\", interpolation=\"nearest\")\n",
    "for i, d in enumerate(selection):    \n",
    "    plt.text(4+8*i,10,\"%s\"%digits.target[d])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Some random digits\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply principal component analysis\n",
    "pca = sklearn.decomposition.PCA(n_components=2).fit(digit_data)\n",
    "digits_2d = pca.transform(digit_data)\n",
    "\n",
    "# plot each digit with a different color (these are the true labels)\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=digits.target, cmap='jet')\n",
    "plt.title(\"A 2D plot of the digits, colored by true label\")\n",
    "# show a few random draws from the examples, and their labels\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now cluster the data\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=10)\n",
    "kmeans_target = kmeans.fit_predict(digits.data)\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=kmeans_target, cmap='jet')\n",
    "plt.title(\"Points colored by cluster inferred\")\n",
    "\n",
    "# plot some items in the same cluster\n",
    "# (which should be the same digit or similar!)\n",
    "def plot_same_target(target):\n",
    "    plt.figure()\n",
    "    selection = np.where(kmeans_target==target)[0][0:20]\n",
    "    digit_seq = [digit_data[s].reshape(8,8) for s in selection]\n",
    "    plt.imshow(np.hstack(digit_seq), cmap=\"gray\", interpolation=\"nearest\")\n",
    "    for i, d in enumerate(selection):    \n",
    "        plt.text(4+8*i,10,\"%s\"%digits.target[d])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Images from cluster %d\" % target)\n",
    "    \n",
    "for i in range(10):    \n",
    "    plot_same_target(i)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now cluster the data, but do it with way too many clusters\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=20)\n",
    "kmeans_target = kmeans.fit_predict(digits.data)\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=kmeans_target, cmap='jet')\n",
    "plt.title(\"20 clusters is too many\")\n",
    "# plot some items in the same cluster\n",
    "# (which should be the same digit or similar!)\n",
    "def plot_same_target(target):\n",
    "    plt.figure()\n",
    "    selection = np.where(kmeans_target==target)[0][0:20]\n",
    "    digit_seq = [digit_data[s].reshape(8,8) for s in selection]\n",
    "    plt.imshow(np.hstack(digit_seq), cmap=\"gray\", interpolation=\"nearest\")\n",
    "    for i, d in enumerate(selection):    \n",
    "        plt.text(4+8*i,10,\"%s\"%digits.target[d])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "for i in range(20):\n",
    "    plot_same_target(i)    \n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Day and night\n",
    "<a id=\"practical\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use a clustering algorithm (choose one from [sklearn](http://scikit-learn.org/stable/modules/clustering.html#clustering)) to cluster a set of images of street footage, some filmed at night, some during the day.\n",
    "\n",
    "The images are available by loading `data/daynight.npz` using `np.load()`. This is a has 512 images of size 160x65, RGB color, 8-bit unsigned integer. You can access these as:\n",
    "\n",
    "    images = np.load(\"data/daynight.npz\")['data']\n",
    "\n",
    "There is also the **true labels** for each image in ['target']. **Obviously, don't use these in the clustering process!**.\n",
    "\n",
    "\n",
    "You should be able to cluster the images according to the time of day without using any labels. The raw pixel values can be used as features for clustering, but a more sensible approach is to summarise the image as a **color histogram**. \n",
    "\n",
    "This essentially splits the color space into coarse bins, and counts the occurence of each color type. You need to choose a value for $n$ (number of bins per channel) for the histogram; smaller numbers (like 3 or 4) are usually good.\n",
    "\n",
    "Make a function that can show the images and the corresponding cluster labels, to test how well clustering has worked; you might also see if there are additional meaningful clusters in the imagery.\n",
    "\n",
    "### Steps\n",
    "1. Load the imagery\n",
    "1. Check you can plot it (use `plt.imshow`)\n",
    "1. Create a set of features using `color_histogram()`\n",
    "1. Try clustering it and plotting the result\n",
    "1. Experiment with clustering algorithms and `color_histogram()` settings and see how this affects clustering performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_histogram(img, n):\n",
    "    \"\"\"Return the color histogram of the 2D color image img, which should have dtype np.uint8\n",
    "    n specfies the number of bins **per channel**. The histogram is computed in YUV space. \"\"\"\n",
    "    # compute 3 channel colour histogram using openCV\n",
    "    # we convert to YCC space to make the histogram better spaced\n",
    "    chroma_img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV) \n",
    "    # compute histogram and reduce to a flat array\n",
    "    return cv2.calcHist([chroma_img.astype(np.uint8)], channels=[0,1,2], mask=None, histSize=[n,n,n], ranges=[0,256,0,256,0,256]).ravel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link to [Manifold Learning: Part II](manifold_2.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
